# ðŸ§  Multi-Label Text Classification using Word2Vec, GloVe, FastText & BERT (PyTorch)

This repository implements a complete end-to-end pipeline for **multi-label text classification** using four different embedding models:

- **Word2Vec**
- **GloVe**
- **FastText**
- **BERT (Sentence-Transformers)**

The project uses the *Consumer Review of Clothing Product* dataset from Kaggle and compares how modern contextual embeddings outperform traditional static embeddings in real-world text classification tasks.

---

## ðŸ“Œ Key Features

âœ” Multi-label classification  
âœ” Four embedding models compared under identical architecture  
âœ” Clean preprocessing pipeline  
âœ” Deep neural network classifier using PyTorch  
âœ” Automatically saved results (CSV + charts)  
âœ” Embedding caching for fast re-runs  
âœ” Fully reproducible single-script pipeline  

---

## ðŸ“‚ Dataset Information

**Dataset Name:** Consumer Review of Clothing Product  
**Source:** Kaggle  
**Link:** https://www.kaggle.com/datasets/jocelyndumlao/consumer-review-of-clothing-product


---

## ðŸ§© Embedding Models Used
ðŸ”¹ Word2Vec

Trained on review corpus (300D)

ðŸ”¹ GloVe

Pretrained vectors (100D / 300D)

ðŸ”¹ FastText

Subword-level embeddings

Trained on corpus or loaded externally

ðŸ”¹ BERT (Sentence-Transformers)

Model used: all-MiniLM-L6-v2

Produces contextual embeddings (384D)

Expected to outperform static embeddings

------
# ðŸ“¥ Preprocessing

Text preprocessing includes:

Lowercasing

Tokenization using NLTK

Removing special characters

Handling missing values

Converting text to embeddings

Multi-label targets are generated using:

Recommended Indicator (binary)

Department Name

Class Name

Fallback labels are used if any columns are missing.


-------
## ðŸš€ Pipeline Overview

### **1. Load and clean dataset**
- Lowercase text  
- Remove punctuation  
- Tokenize  
- Handle missing values  

### **2. Build multi-label targets**
- Convert `Recommended IND`
- One-hot encode `Department Name`
- One-hot encode `Class Name`
- If missing â†’ fallback `Cloth_class` + `Cons_rating`

### **3. Generate embeddings**
- **Word2Vec:** trained on corpus  
- **GloVe:** pretrained (optional download)  
- **FastText:** trained on corpus, handles OOV  
- **BERT-SBERT:** contextual semantic vectors  

### **4. Train PyTorch classifier**
A 2-layer neural network:
Loss = `BCEWithLogitsLoss`  
Optimizer = `Adam`  

### **5. Evaluate using**
- Micro F1  
- Macro F1  
- Precision  
- Recall  
- AUC per label  

### **6. Export results**

Saved to: csv file

----------------
# ðŸ“ˆ Expected Results

| Embedding Model  | Expected Performance  |
| ---------------- | --------------------- |
| **BERT (SBERT)** | â­ Highest performance |
| FastText         | Very strong           |
| Word2Vec         | Good baseline         |
| GloVe            | Solid older method    |

-----------
# ðŸ“Œ Key Insights

Contextual models > Static averages

FastText excels in noisy real-world datasets

Domain-trained Word2Vec/FT improves significantly

Multi-label formulation is more realistic than sentiment-only models






