# ðŸ§  Multi-Label Text Classification using Word2Vec, GloVe, FastText & BERT (PyTorch)

This repository implements a complete end-to-end pipeline for **multi-label text classification** using four different embedding models:

- **Word2Vec**
- **GloVe**
- **FastText**
- **BERT (Sentence-Transformers)**

The project uses the *Consumer Review of Clothing Product* dataset from Kaggle and compares how modern contextual embeddings outperform traditional static embeddings in real-world text classification tasks.

---

## ðŸ“Œ Key Features

âœ” Multi-label classification  
âœ” Four embedding models compared under identical architecture  
âœ” Clean preprocessing pipeline  
âœ” Deep neural network classifier using PyTorch  
âœ” Automatically saved results (CSV + charts)  
âœ” Embedding caching for fast re-runs  
âœ” Fully reproducible single-script pipeline  

---

## ðŸ“‚ Dataset Information

**Dataset Name:** Consumer Review of Clothing Product  
**Source:** Kaggle  
**Link:** https://www.kaggle.com/datasets/jocelyndumlao/consumer-review-of-clothing-product


---

## ðŸ§© Embedding Models Used
ðŸ”¹ Word2Vec

Trained on review corpus (300D)

ðŸ”¹ GloVe

Pretrained vectors (100D / 300D)

ðŸ”¹ FastText

Subword-level embeddings

Trained on corpus or loaded externally

ðŸ”¹ BERT (Sentence-Transformers)

Model used: all-MiniLM-L6-v2

Produces contextual embeddings (384D)

Expected to outperform static embeddings

------
# ðŸ“¥ Preprocessing

Text preprocessing includes:

Lowercasing

Tokenization using NLTK

Removing special characters

Handling missing values

Converting text to embeddings

Multi-label targets are generated using:

Recommended Indicator (binary)

Department Name

Class Name

Fallback labels are used if any columns are missing.


